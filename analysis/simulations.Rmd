---
title: "Simulation Examples"
author: "Nathanael Aff"
date: 2017-05-10
output: 
  html_document: 
    code_folding: hide
    toc: true
---

<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->

```{r read-chunk, include=FALSE, cache=FALSE}
knitr::read_chunk("../analysis/chunks.R")

```

<!-- Update knitr chunk options -->
```{r knitr-opts-chunk, include=FALSE}

```

<!-- Insert the date the file was last updated -->
```{r last-updated, echo=FALSE, results='asis'}
```

<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
```{r code-version, echo=FALSE, results='asis'}

```

### Introduction 

The $\varepsilon-$complexity of a time series is computed by successively downsampling and approximating the original time series. The initial implementation of the $\varepsilon-$complexity procedure used piecewise polynomials as the approximation method. We implemented the procedure in the `R` language using three approximation methods, B-splines, cubic splines, and interpolating subdivision. We implemented the latter method  in `R` 
while the spline approximation methods used existing libraries. 

The purpose of adding additional approximation methods was to improve the performance of the complexity coefficients in segmentation and classification tasks. We found that improved approximation did not lead to improved classification performance. Cubic splines were used in later applications because of the computational efficiency of the method. 

```{r sim-load}
knitr::read_chunk("../code/sim-example.R")
```


```{r sim-create, fig.cap = "Single time series from each simulation group."}
<<sim-example-create>>
<<sim-series>>
```
### Diagnostic plots
In the theory developed by Piryatinska and Darkhovsky[^1], the $log-log$ regression of approximation errors taken at each downsampling level $S$ should be roughly linear. The two parameters defining this linear fit are the complexity coefficients.The implementation of the three approximation methods was checked to determine that this linear relationship held. Below is the plot for the interpolating subdivision method. Diagnostic plots for the other methods produced similar results.

(An outline of the estimation procedure is [here](ecomplexity-procedure.html))

```{r sim-plot, fig.cap = "Diagnostic plots for interpolating subdivision."}

<<sim-example-plot>>

```

### Approximation method performance

 We hypothesized that lowering approximation error would improve classification accuracy when the complexity coefficients were used as features.  

A plot of the simulations in the feature space of the two complexity coefficients shows that the complexity coefficient generated by each method lie in roughly the same area in the coefficient space. The plot of the simulations in the complexity coefficient space shows that the same process -- ARMA, logistic map, and fBM -- appear separable for each method. That is, for those three processes the two groups of simulations could be separated by a line in the coefficient space.

```{r sim-features0}
knitr::read_chunk("../code/ecomplex-plot-feature-space.R")

```
```{r sim-features1}
devtools::load_all()
<<sim-feature-space>>
```

For the following test two groups were formed consisting of 30 simulations of each random process or function. Each process had an initial set of parameters and samples of the process were generated by randomly perturbing these parameters within a small window. 


```{r sim-features, fig.cap = "Simulations plotted in the features space of the complexity coefficients A and B."}
<<ecomplex-feature-space>>
```

The B-spline produced the lowest approximation error for all methods but did not out perform the other methods in the classification task. 


```{r approx-errors}
  devtools::load_all()
  prefix = "ecomplex"

  err_means <- readRDS(cache_file("all_errs", prefix))
  rf_errs <- readRDS(cache_file("rf_errs", prefix))
  
  err_means <- data.frame(err_means)
  names(err_means) <- names(rf_errs)
  row.names(err_means) <- row.names(rf_errs)
  knitr::kable(err_means, caption = 'Approximation error for each method.')

```

```{r class-errors}
  knitr::kable(rf_errs, caption = 'Classification error for each method.')
````

The complexity coefficients were used as features in a random forest classifier, the same classifier we used in later applications. No approximation method was clearly dominant but the methods with higher approximation errors performed as well or better than the B-spline method. While all three approximation methods are similar -- essentially, they all use lower order polynomial approximation -- the results do indicate improved approximation doesn't lead to improved performance in classification. 

For later applications, the cubic splines method was used because of the computational efficiency of the implementation. 


[^1]: [Epsilon-complexity of continuous functions](https://arxiv.org/abs/1303.1777)

### Session information

<!-- Insert the session information into the document -->
```{r session-info}

```
