

\chapter{Background}
    
\section{Epsilon-Complexity} 
  

  The term 'complexity' has been used to describe a diverse set of both mathematical and natural phenomena. Our use of the term complexity to describe a property of a continuous is related to Kolmogorov's definition of the complexity of a discrete sequence. Kolmogorov complexity characterizes the regularity of a
   sequence or string by the size of the program needed to output that sequence. For example, we can take as our sequence a natural number and let our 'program' be the representation of that number in scientific notation.
    Then we can express 1,000,000 in 2 digits as 1E6 while the prime 7919 could not be similarly compressed \cite{vitanyi1993}. For
    this case, 1,000,000 is less complex 7919. 

  The approach taken by Darkhovsky and Piryatinska in defining the $\varepsilon$- a complexity of a continuous function agrees with Kolmogorov's definition of complexity\cite{darkhovsky2013}. 
   Roughly speaking, the complexity of a continuous 
   function is measured by the amount of information that is needed to reconstruct that function within some error $\varepsilon$. 
   The procedure used to compute $\varepsilon-$complexity coefficients makes successive approximations of
   a function with a more sparsely sampled set of points at each iteration. The two $\varepsilon$-complexity coefficients are the
   parameters of the linear relationship between 
   the log of the approximation error and the log of the proportion 
   of points used for each approximation.

  
  We begin with the formal definition of the $\varepsilon-$complexity of a continuous function followed by the primary theorem of \cite{darkhovsky2013} which shows that, for the H\"older class of functions, $\varepsilon-$complexity can be characterized by a pair of real numbers. To extend the definition of $\varepsilon-$complexity to the case of a discrete function, the function is taken to be the restriction of a continuous function to a discrete grid and the theorem is adjusted to this case. We also present the estimation procedure used to compute the $\varepsilon-$complexity coefficients. 

  Let $x(t)$ be a continuous function defined on the 
  unit interval $ \mathbb{I} = [0,1]$. Let $\norm{\cdot}$ be a norm on the function and we assume $\norm{x(t)} = 1$, that is, the function has been normalized 
  by taking $x(t)/\norm{x(t)}.$ Given some family of approximation 
  methods $\mathcal{F}$, let $\hat x(\cdot)$ be an approximation
  of $x(t)$ reconstructed from regularly samples spaced at intervals $h$. Then the absolute recovery error is defined 
  \begin{align}
    \delta^{\mathcal{F}}(h) = \inf_{\hat x \in \mathcal{F}} 
    \sup | x(t) - \hat x(t)|.
  \end{align}
  That is, $\delta^{\mathcal{F}}(h)$ is the minimum error overall 
  all methods in $\mathcal{F}$ for a given spacing $h$.
  We assume that $x(t)$ does not admit a perfect 
  reconstruction. Now we define the following as a function 
  of the error $\varepsilon$:
    % We will assume that $x(t)$ does not permit a perfect 
    % reconstruction for any $\hat x(\cdot) \in \mathcal{F}.$
    \begin{align*}
      h_x^*(\varepsilon, \mathcal{F}) = \begin{cases}
      \inf \{ h \leq 1 : \delta^{\mathcal{F}}(h) > \varepsilon \} & \text{if} 
      \{ h : \delta^{\mathcal{F}}(h) > \varepsilon \} > 0  \\ 
        1, &  \text{if no such } \hat x(t) \in \mathcal{F} \hspace{1em}\text{exists.}
    \end{cases}
  \end{align*}
  In words, the function $h^*(\varepsilon, \mathcal{F})$ is the minimum grid spacing $h$ needed to approximate $x(t)$ within an absolute error not larger than $\varepsilon$. We can now define $\varepsilon-$complexity. 
  \begin{defn}{\textbf{Epsilon-complexity.} }\label{def:ecomplexity}
  The number
  \begin{align}
    \mathbb{S}_x(\varepsilon, \mathcal{F}) =  
    -\log h_x^*(\varepsilon, \mathcal{F}) 
  \end{align}
  is the $(\varepsilon, \mathcal{F})-$complexity, 
  or simply, $\varepsilon-$complexity of the function $x(\cdot)$. 
  \end{defn}

  We will be referring variously to the H\"older condition or 
  H\"older continuity of a function which we now define.
  \begin{defn}{\textbf{H\"older Continuity.} }\label{def:holder}
    A function $x(t)$ is said to be H\"older continuous if
  there exists non-negative constants, $C, \alpha$ such that 
  \[
      |x(t+h) - x(t)| \leq C|h|^{\alpha}
  \]
    for all $t$ and $h$.
  We refer to this as the H\"older condition with H\"older 
  exponent $\alpha$ or say a function is H\"older $\alpha$.
  \end{defn}
   Darkhovsky and Piryatinska have shown that for almost every
  individual H\"older class function, and given a rich 
  enough family of approximation methods,  
  the epsilon complexity of a function has an approximately linear
  relationship to $\log \varepsilon$,
  \begin{align} \label{eq:linear-ecomplex}
      -\log h_x^*(\varepsilon, \mathcal{F}) \approx \mathbb{A} + 
    \mathbb{B}\log \varepsilon  
  \end{align}\cite{darkhovsky2013}.
  
  This relationship characterizes the 
  $\varepsilon-$complexity of a H\"older function. An analgous 
  relationship is used 
  for the estimation of $\varepsilon$-complexity for 
  discrete time series.
  In practial applications any function or time series 
  is acquired as some discrete set of samples, which we
  will assume to be taken at regular intervals. We 
  consider the function in the discrete case 
  to be a continuous function
  restricted to a uniform grid. Then 
  $h^*(\epsilon, \mathcal{F})$, which we denote here 
  $h^*(\epsilon)$, is the proportion of sampled points,
  and $\frac{1}{h^*(\varepsilon)}$ is the 
  number of sampled points, 
   needed to approximate some function $x(t)$ within 
  an absolute error $\varepsilon$. Let $n$ be the total number of points 
  in the initial sample, then the 
  proportion of points needed for reconstruction with error not
  larger than $\varepsilon$ is defined 
  \begin{align*}
     S(\cdot)  = \frac{\left\lfloor h^*(\varepsilon)n 
     \right\rfloor }{n}.
   \end{align*} 
   \begin{defn}{\textbf{Epsilon-complexity (discrete).}} 
     The $\varepsilon-$complexity of a function 
     represented by a set of values sampled on 
     a uniform grid is $-\log S(\cdot)$.
   \end{defn}
  It follows from defintion \ref{def:ecomplexity} that
  \begin{prop}
    \[
      -\log S(\varepsilon, \mathcal{F}) \to \mathbb{S}_x(\varepsilon, \mathcal{F}) 
    \]
    as $n \to \infty$.
  \end{prop}
  For the discrete case, the relation analogous to \ref{eq:linear-ecomplex}, after substitution of $\log S$ for $\mathbb{S}_x(\varepsilon, \mathcal{F})$, is give by 
  \begin{align}\label{eq:discrete-ecomplexity}
     \log \varepsilon  \approx A + B \log S.
   \end{align} 
  When computing the complexity coefficients, some set of grid spacings 
  $h$, corresponding to the proportions $S_h$, are used to 
  estimate the linear relation in \ref{eq:discrete-ecomplexity}. 
  For each spacing $h$ we find the minimum error approximation 
  at that spacing.
  The parameters of this linear relationship, $A, B$, are the 
  $\varepsilon-$complexity coefficients or simply the complexity coefficients. 

   In applications, the discrete function $x_t$ is given by 
   some set of samples which we assume to be uniformly
   sampled at an initial interval $h_0$.
   Approximations are made using samples taken on some set of wider intervals $\{ h \}$. 
  For some set of integers $\{ \alpha \}, \alpha > 1$, 
  the function $x_t$ is approximated using the function values 
  downsampled to a grid with spacing $h = \alpha \cdot h_0$. 
  These approximations are taken from one or more families of methods $\mathcal{F}$, for example, the family of piecewise polynomials. 
  For a given downsample determined by $\alpha \cdot h_0$, 
   the minimum mean-squared error of all approximation
   methods $\mathcal{F}$ 
   is then added to the set of errors, 
   $\varepsilon_{\mathcal{F}, h}$. 
   Finally, setting $S_h = \frac{1}{h}$, a least squares linear model is fit to the set of errors   $ \{ \varepsilon_{\mathcal{F}, h} \}$ regressed on the proportion of function values 
   $\{ S_h \} $:
  \[
      \log  \varepsilon_{\mathcal{F}, h} 
      \sim A + B \log S_h.
  \]
  The parameters of the linear model $A$ and $B$ are the 
  the complexity coefficients.

   \begin{algorithm}[htb]
    \label{alg:ecomplexity}
  \caption{Estimation of $\varepsilon-$complexity \label{alg:ecomplex}}
  \DontPrintSemicolon
  \SetAlgoLined
  \SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
  \Input{$X$ a regularly sampled time series of length $N$.} 
  \Input{$ \mathcal{F} $ a set of approximation methods $f$.}
  \Input{$ \mathcal{H} $ the set of spacings $h =  \alpha \cdot h$ 
      for $\alpha \in \{ \alpha \}$.}
  \Output{The complexity coefficients $A,B$.}
  % \tcp{initialize array}
   % Initialize the array epsilons $\leftarrow [0]$   \;
  \BlankLine 
 \ForEach{$h$ in $H$} {    
    % \tcp{initialize array}    j
    % mse $\leftarrow [0]$ \;
    \ForEach{$f$ in $\mathcal{F}$} {
      % \tcp{Compute the approximation error}
        Compute the approximation error \\ 
        $\varepsilon_{h,f} \leftarrow 
       \frac{1}{N}|f_h - X_{h} |^2$.  \;
      % mse\leftarrow \varepsilon_{h,f}$
    } 
     Find minimium error over all $f$  \\
     epsilons$_h$ $\leftarrow \min \varepsilon_{h,f}$. \;
  }
  Fit a least squares linear model \\
   $A,B$ $\leftarrow$ lm( $\{ S_h \}  \sim \{$ epsilons$_h$ $\}$). \;
  \end{algorithm}

% \caption{$\varepsilon-$complexity}

%   \end{algorithm}
% \usepackage[ruled,vlined]{algorithm2e}
\section{Time Series}
  
  Time series are a univariate set of discrete observations index by time. In the context of statistical modeling, 
  time series are taken to be observations of some stochastic process. 
  A stochastic process is a set of random 
  variables, $X(t)$ or $X_t$, parametrized by time, 
  $\{ X_t: t \in \{ 1,2,3,...,N\}\}$.  
  Then a time series is a sample path of a 
   stochastic process which we denote with the lower case $x_t$. 
   We note here that when considered 
   in the context of $\varepsilon-$complexity, we 
   ignore the probabilistic interpretation of a 
   time series and instead
   treat the time series $x_t$ as the restriction of 
   a continuous function $x(t)$ to a uniform grid.

   We begin by defining some standard characteristics of time series. 
   A \textit{strictly stationary} 
   stochastic process is one 
   whose distribution is independent of time $t$. 
   \begin{defn}{\textbf{Strict stationarity.}} A strictly stationary time series 
   $\{ X_t, t \in \Z  \}$ is a time series
   such that for all $h$ 
   \[
     \{ X_1, X_2, ..., X_n \} \hspace{1em}
     \text{and}
      \hspace{1em}
     \{ X_{1+h}, X_{2+h}, ..., X_{n+h} \}
   \]
   have the same joint distribution for all $n \geq 1$ and $h$, 
   $n, h \in \Z$ \cite{fan2003}.
   \end{defn}
  Strict stationarity is difficult to determine in practice 
  and we will be using the term stationary to refer to 
  weak stationarity.
  \begin{defn}\label{def:stationary}{\textbf{Weak stationarity.} }
   Where it exists, let $\mathop{E}(X_t)  = \mu_{t}$ be the mean function of a time series.
   A  time series with finite variance and 
  mean $\mu_t$ is called 
   \textit{weakly stationary} if it satisifies the following 
   conditions: 
  \begin{align*}
    &\mathop{E}(X_t^2) < \infty, \\
    &\mathop{E}(X_t)  = \mu_t \hspace{1em} \text{is a constant 
    independent of t },\hspace{1em} \\
    &\text{Cov}(X_t, X_{t+ h }) \hspace{1em}\text{ is independent
    of } t \text{ for each } h .\hspace{1em} 
  \end{align*}   
  \end{defn}
  
  Covariance and correlation quantify how two or more random 
  variables jointly vary. For time series, this   
  joint variablilty of a time series with itself at 
  various time lags $h$ is measured by the the 
  autocovariance and autocorrelation of the time series.
  \begin{defn}{\textbf{Covariance function.}} 
  \label{eq:auto_covariance}
  The  \textit{autocovariance or covariance} of a stochastic 
  process with mean $\mu_t$ 
  is defined
  \begin{align*}
    \sigma(h) = \text{Cov}(X_{t}, X_{t + h})
                = E[(X_{t}-\mu_t)(X_{t + h} - \mu_{t + h})].  
   \end{align*}
   In the case of a zero mean function this simplifies to
   \begin{align*}
      \sigma(h) = E[(X_{t})(X_{t + h}]).  
   \end{align*}
    The \textit{autocorrelation} function of the time series $X_t$ is
    the normalized autocovariance and is defined
    \begin{align*}
       \rho(h) \equiv \sigma(h)/\sigma(0) \equiv \text{Corr}(X_{t + h}, X_t), 
       \hspace{2em} h \in \{ 1,2,3,... \}.
     \end{align*} 
  \end{defn} 
  It follows that for a stationary, mean zero stochastic process with 
  $\sigma^2 = 1$ the autocorrelation and autocovariance function 
   are equivalent. 

  So far we have defined time domain properties of time series. The \textit{spectral density}
  or \textit{power spectrum} is a frequency domain 
  decomposition of a time series. In particular, 
  the spectral density function expresses the variance or power of a signal over a discrete or continuous frequency domain. The Wiener-Khintchine theorem relates the autocorrelation of a time series to its 
  spectral density function. 
  
\begin{thm}{\textbf{(Wiener-Khintchine theorem)}}
  For a real-valued function defined on the integers 
  $\{ t = 0, \pm 1, \pm 2, ... \}$ then $\rho(t)$ 
  is the autocorrelation function of a stationary 
  time series if and only if there exists a 
  symmetric distribution function $F$ on $[-\pi, \pi]$
  \begin{align}
      \rho( t ) =  \int_{-\pi}^{\pi} e^{i t \omega}dF(\omega)d \omega \hspace{1em}.
    \end{align}
    where $F$ is called the normalized distribution function
    of the time series. If $F$ has a density function $f$
    then we call $f$ the normalized spectral density function
    and 
    \begin{align}
      \rho(t) =  \int_{-\pi}^{\pi} e^{i t \omega}f(\omega)d \omega \hspace{1em}.
    \end{align}\cite{fan2003}
\end{thm}
   
  To simplify, the autocorrelation function $\rho(t)$ 
  and the spectral density function $f$ are 
  invertible Fourier pairs when they satisfy the 
  necessary conditions. 
  % Time series in practice are given by a finite sample
  % with finite autocovariance
  % and the spectral decomposition of the sample can always 
  % be estimated.
  A time series with a slowly a decaying autocorrelation function is said to exhibit \textit{long-range dependence}. In theory, a stochastic
  process with long-range dependence would not have an infinitely summable
  autocovariance function. In practice, a time series is given by a finite sample on which a spectral decomposition can always be performed.

Some stochastic processes can be defined by the distribution 
of their \textit{increments}. The increments of a stochastic process are defined as $\{ X_t - X_{t+h} \}$. The variance of the increments
of a stochastic process is called the \textit{variogram}, 
although the term is more commonly used in reference to 
spatial processes or random fields, that is, stochastic processes 
indexed by more than one variable. 
\begin{defn}{\textbf{Variogram.} }\label{def:variogram}
Let $X_t$ is a zero mean stochastic process with 
stationary increments. The variogram of $X_t$ 
is  defined
\begin{align*}
  \gamma^2(h) = \frac{1}{2}\mathbb{E}\left(X_t - X_{t + h} \right)^2.
\end{align*}
\end{defn}
% For Gaussian processes, 
% The variogram relates to the the covariance function  $\sigma(h)$ via\cite{gneiting2012} 
% \[
%   \gamma^2(h) = \sigma(0) - \sigma(h).
% \]
Let $N(h)$ be the total of all pairs of points in 
a time series at distance
$h$ and denote the size of that st $|N(h)|$. Then the \textit{empirical variogram} is defined 
\begin{align*}
  \hat \gamma(h) =  \frac{1}{2|N(h)|}  \sum_{i,j\in N(h)} |x_i - x_j|^2.
\end{align*}

\section{Fractal Dimension}

Although fractal(or fractional) dimension is a general measure on sets,
we will be considering only those sets formed by the graph of a function $f$ or a sample path drawn from a stochastic process. Fractal dimension 
coincides with the usual sense of dimension for smooth manifolds; for example, the fractal dimension of a line in $\R^1$ is 1. 
Since we consider only graphs
 $\Gamma:\{ (x, f(x)) \}$ in $\R^2$,
the fractal dimension of these graphs should take on 
values between $[1,2)$.

The Hausdorff measure of a set $E$, roughly speaking, 
measures the size of covers of that set 
as the diameter of covering elements
$\{ U_i \}$ approaches zero. We define the diameter 
of a set $U$ denoted $|U|$ as 
$\sup \{ | x- y| : x, y \in U\}$, the greatest distance 
between any two points in the set. A collection 
$ \{ U_i \}$ is 
a cover of $E$ if $E \subset \bigcup_i^{\infty} U_i$
where we assume $\{ U_i \}$ is countable. 

\begin{defn}{\textbf{Hausdorff Dimension.} }
Let all elements in $ \{ U_i \}_{\delta}$ be a
countable subset of $\{ U_i \}$ with diameter greater 
than $\delta$ such that $\{ U_i \}_{\delta}$ cover 
some set $E \subset \R^n$. The the Hausdorff measure 
$\mathcal{H}_{\delta}^{\alpha}$ for any $\delta > 0$ 
is defined 
\begin{align*}
  \mathcal{H}_{\delta}^{\alpha} = \inf 
    \left[ \sum_i | U_i |^{\alpha} : \{ U_i \} \hspace{0.5em}\text{ is  a cover of  }\hspace{0.5em} E \right]
\end{align*}  
and 
$$
\mathcal{H}^{\alpha(E)}=\lim_{\delta\to 0} H^{\alpha(E)}_\delta.
$$

\end{defn}

For $0 < \alpha < 1$, $\mathcal{H}^{\alpha}$ is 
either $0$ or $\infty$ and there exists a critical point 
$\alpha$ at which $\mathcal{H}^{\alpha}$ switches from $0$ to 
$\infty$. This critical point is  
the Hausdorff dimension of the set $E$ 
which we denote $\dim_H E$ \cite{falconer2003}. 

\For many sets, the Hausdorff dimension is difficult to determine and a number of related measures are used as more tractable alternatives. 
The box-counting dimension is one these and, for sets in $\R^2$, 
it counts the number of squares of length $\delta$ required
to cover a set. It also provides an upper bound for the 
Hausdorff dimension of a graph. The box-counting dimension 
is defined as
\[
  \dim_B E = \lim_{\delta \to 0} \frac{ \log N_{\delta}(F)}
  {- \log \delta}
\]
where $N_{\delta}$ is the smallest number of sets 
that cover $E$ of side length $\delta$. In what follows 
we occasionally refer more 
loosely to the fractal dimension of a function 
or time series by which we mean, more properly, the 
fractal dimension of the graph of the function.  

There is a general relation between the 
H\"older-$\alpha$ functions and the fractal dimension of 
a continuous function on $\R$. The H\"older condition is an upper bound on the box-counting dimension.
\begin{prop}
  Let $f:[0, 1] \to \R$ be a continuous function 
  and 
  \[  
    |f(t) - f(t + h)| \leq ch^{\alpha} \hspace{1em} 
    (c > 0, 0 \leq \alpha \leq 1)
  \]
  then $dim_B f \leq 2 - \alpha $\cite{falconer2003}.
\end{prop}

We use a variogram estimator of fractal dimension in Chapter 4. 
This method estimates fractal dimension by taking the log-log regression of the empirical variogram against increments $h$. This method is recommended as a robust estimator of fractal dimension
 by Gneiting in \cite{gneiting2012}.
One justification for the estimation method is the relation between the variogram of certain stochastic processes and its fractal dimension.
\begin{prop}\label{prop:gauss-variogram}
  Let $X_t$ be a Gaussian process with variogram satisfying
  \begin{align}\label{eq:gauss-variogram}
    \gamma(h) \sim|ch|^{\alpha},
  \end{align}  
  then the Hausdorff dimension of the sample path of 
  the Gaussian process, $dim_H \Gamma(X_t)$, is almost surely $2 - \alpha$. For Gaussian processes satisifying 
\ref{eq:gauss-variogram}, the H\"older exponent of its sample paths
is almost surely $\alpha$\cite{gneiting2012} \cite{orey1970}. 
\end{prop} 

\section{Simulations}

In the following two chapters we use a set of simulations to test our implementation of $\varepsilon-$complexity
estimation and to explore
properties of the $\varepsilon$-complexity coefficients. 
These simulations include both deterministic functions
and stochastic processes. We introduce these
processes and functions here and describe some of their properties.

% \textbf{Logistic Map}
The \textit{logisitc map} is a single parameter nonlinear deterministic discrete difference equation defined
\[
  x_t = r(x_{t-1})(1 - x_{t-1}).
\]
For values of the parameter $r$ above approximately 3.6 but less than 4, the trajectory of the 
logistic map exhibits chaotic behavior and the graph of the function
changes significantly for small perturbations of the $r$ parameter. 

% \textbf{ARMA process}

The \textit{ARMA} model is a widely used stochastic model
that combines autoregressive (AR) and moving average (MA)
models. The AR component expresses an observation at time 
$t$ as a linear combination
of preceding observations plus a random error component:
\begin{align*}
  X_t  = b_1 X_{t-1} + \cdots + b_p X_{t-p} + \varepsilon_t
\end{align*}

The number of previous observations $p$ is the order of the 
model, denoted $AR(p)$. The random error, $\varepsilon_t$, 
is assumed to be independent, identically distributed (IID) observations from a standard normal distribution with 
mean 0, variance $\sigma^2$ and covariance 
$Cov(X_i, X_j) = 0, i \neq j$. The latter is called 
white noise or Gaussian noise and the errors are said to be 
drawn from a white noise process 
$\{ \varepsilon_{t_i}, \varepsilon_{t_j}\} \sim WN(0, \sigma^2)$. 

The moving average(MA) process of order $q$ models
the a time series as a linear combination of $q$
samples $\{ \varepsilon_t \} \sim WN(0, \sigma^2)$  
\begin{align}
  X_t = \varepsilon_t + a_1 \varepsilon_{t-1} + 
\cdots + a_q \varepsilon_{t-q}.
\end{align}
For our simulations we use parameters that define stationary ARMA$(p,q)$ processes.

% \textbf{FARMIA}
A stationary ARMA process has an autocorrelation  
function $\rho(h)$ that decays at an exponential rate 
as $|h| \to \infty$ \cite{fan2003}.   
We include several functions that have a slowly decaying autocorrelation function, a characteristic of processes with long-range dependence. The first of these is the fractionally integrated ARMA process or FARIMA process. 
The ARIMA process is model whose integral 
differences $d$ are a ARMA(p,q) processes,
denoted ARIMA$(p,d,q)$.  
The FARIMA model modifies the ARIMA process by 
allowing for fractional values $d$\cite{fan2003}.

% \textbf{Weierstrass Function}
The \textit{Weierstrass} function is 
a well-known example of a continuous but nowhere differentiable function. The Weierstrass function can be written in several forms. Here we use the form that isolates the parameter $\alpha$ corresponding to the H\"older exponent of the function. 
\begin{defn}\textbf{Weierstrass function.} \label{def:weierstrass}
  The \textit{Weierstrass} function defined
  \begin{align}
    W_{\alpha}(x) 
  \hspace{1em}= \sum_{n = 0}^{\infty} b^{-n \alpha} \cos(b^n \pi x)
  \end{align}
  is a continuous 
  periodic nowhere differentiable function 
  that is H\"older-$\alpha$:  
  \[
  \left| W_{\alpha}(x) - W_{\alpha}(y) \right| 
  \leq C \left| x - y \right|^{\alpha}.
\]  
\end{defn}
The parameter $\alpha$ determines both the fractal and 
Hausdorff dimension of the Weierstrass function. 
The Weierstrass function has box-counting dimension equal to $2-\alpha$, providing the upper bound for the Hausdorff dimension \cite{falconer2003}. That the lower bound of the Hausdorff dimension is also equal to $2-\alpha$ was only recently proved by Shen \cite{shen2015}. 

A variant of the Weierstrass function is \textit{ the random-phase 
Weierstrass} function.
The random phase function has the same form as the 
Weierstrass function with an added phase, $\phi$, drawn from 
a uniform distribution:
\[
  W_{\alpha}(x) 
   = b^{-n \alpha} \cos(b^n \pi x  + \phi_n), \hspace{1em} 
   \phi_n \sim \hspace{1em} \text{unif}(0,1).
\] 
The Hausdorff dimension of the random phase variant Weierstrass function has also been proved to be $2- \alpha$ \cite{hunt1998}. 
See Figure \ref{fig:weierstrass} in the following chapter 
for illustrations of both Weierstrass functions.

Brownian motion and \textit{fractional Brownian motion(fBm)} 
are both Gaussian processes that can be defined in terms of their increment process. 
Brownian motion has normally, independently 
distributed increments and fractal dimension 
$1 \frac{1}{2}$. On the other hand, fBm has 
 \textit{dependent} increments with
\[
  E[(X_{t} - X_{t +h} )^2] = h^{2\alpha}.
\]
When $\alpha = \frac{1}{2}$ fBm is equivalent to 
Brownian motion. 

For fBM, the parameter $\alpha$ determines both 
the fractal dimension, Hurst parameter, and the 
H\"older exponent of the sample paths.
When $\alpha > \frac{1}{2}$ fBm has lower fractal dimension and exhibits long-range dependence with $\alpha$ corresponding
to the Hurst coefficient of the process. 
When $\alpha < \frac{1}{2}$ sample paths of fBM have a 
higher fractal dimension and a lower Hurst parameter.

% [Image Weierstrass function, random weierstras]
In contrast to fBm, the \textit{Cauchy Process}, 
has two parameters which separately control 
the long-range dependence of the function 
and its local behavior or fractal dimension \cite{gneiting2004}.
The Cauchy process is a stationary Gaussian process determined by
 its autocorrelation function:
\[
   \rho(t) =  (1 - |ct|^{\alpha})^{-\beta/\alpha}, 
   \hspace{1em} \alpha \in (0,2]; \beta > 0.
\]
The Cauchy process parameter $\alpha$ determines the fractal 
dimension $D$ of the process via 
  \[
    D = n + 1  - \frac{\alpha}{2},
  \]
and for $\beta \in (0,1)$ the Cauchy process has long-range 
  dependence with Hurst parameter 
  \[
    H  =   1 - \frac{\beta}{2}.
  \]

\section{Approximation Methods}

In calculating the complexity coefficients we use some set 
of approximation methods $\mathcal{F}$.
In the initial implementation, the complexity coefficients were estimated using piecewise polynomials of
up to degree 5. 
For our implementation, we added several methods to the procedure for estimating $\varepsilon-$complexity. 
In Chapter 4, we compare the accuracy of these 
approximation methods on our set of simulations.
The $\varepsilon-$complexity coefficients are estimated
for two groups of simulations using each approximation
method. We compare both the average minimum approximation errors of each method and the classification accuracy of classifiers trained 
on the complexity coefficients as estimated by each method. Here 
we briefly describe the three approximation methods and some of the
properties.

Splines are piecewise polynomials joined on some set of knots, or intersection points, with the additional constraint that the spline function of order $n$ has continuous derivatives of order
$n-1$ at each knot. Two spline methods were used in our implementation, cubic splines and basis splines of order up to 5.

Basis or B-splines of order $n$ are smoothly joined splines that form a basis for the space of $n$-order splines. That is, any $n$-order spline can be written as a unique linear combination of B-splines. For regression splines, an intermediate
design matrix is used in the calculation of the final approximation. 
This means the computational complexity and memory requirements of the method can grow at a greater than the quadratic rate with the number of basis elements used. 

The cubic spline method is a spline method but the
splines are restricted to degree 3 piecewise polynomials. For cubic splines, the design matrix is tridiagonal, having entries only on its diagonal and two off-diagonals. An LU type decomposition for tridiagonal matrices allows for linear time cubic spline interpolation \cite{dahlquist08}. In practice, this means cubic spline interpolation with a fixed number of knots is much less costly in terms of both memory and computational efficiency compared to B-spline regression methods that use
matrix storage and matrix arithmetic.

For a third method we implemented an interpolating 
subdivision technique which we will refer to 
as the lifting scheme after Sweldens. Sweldens gives
a practical introduction to interpolating subdivsion 
in \cite{sweldens1996}. To 
convey the basic approach, we consider a simple 
example of repeated interpolation. Given a set 
of regularly sampled points $\{ s_{0,k} \}$ with 
$k \in \Z$ points a linear interpolation of the 
point $x_{j+1, 2k + 1}$ is the average of the 
adjacent points $s_{j,k}$ and $s_{j,k+1}$. A
recursive formula for averaging and interpolating 
mid-points at stage $j+1$ is given by
\begin{align*}
      s_{j+1, 2k} &= s_{j,k} \\
      s_{j + 1, 2k+1} &= 1/2(s_{j,k} + s_{j,k+1}).
 \end{align*}
 For this example, repeated iterations would simply interpolate two points with a line. If the sampled function was linear, the interpolation would be an exact reconstruction. 

The lifting scheme is similar to wavelet interpolation methods. Unlike wavelet based interpolation, though, the lifting scheme can be extended to interpolating
irregularly spaced points since uniform spacing is not a condition for polynomial interpolation. For the $\varepsilon-$complexity algorithm the pattern of interpolation changes based on the grid spacing $h$ at which samples are taken. 
However, for a set interpolation pattern the  
polynomial interpolant needs to be computed 
only once. Neville's algorithm is used 
to compute a set of $2*(n-1)$ weights for an 
$n-$order polynomial interpolation. To use the 
simple example order 2 or linear interpolation above, 
the weights to compute $x_{j+1, 2k}$ at the 
mid-point of $s_{j, k}, s_{j, k+1}$ are 
$(1/2, 1/2)$. Then $x_{j+1, 2k}$ is calculated
\[
   \langle 1/2 , 1/2 \rangle \cdot 
   \langle s_{j, k}, s_{j, k+1} \rangle = x_{j+1, 2k}.  
\]
These weights, $(1/2, 1/2)$, can be thought of 
as a time-domain filter. For our approximation, similar 
filters were computed for each order polynomial and interpolation
pattern. Interpolation of a given point is then accomplished with 
a similar inner product, with adjustments made for boundary points.

For our implementation 
polynomials of orders ${1,3,5}$ were used for interpolation and for each order polynomial three filters were computed for each distinct
interpolation pattern. Since these filters needed only to be generated once, the interpolation
method is $O(n)$ or linear in the number of inputs. 

