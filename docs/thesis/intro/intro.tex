% \documentclass[thesis]{subfiles}

% \begin{document}
% main.tex, to be used with thesis.tex
% This contains the main work of your thesis.

% \include{}

\chapter{Introduction}

Many natural phenomena generate time series that
exhibit complex functional and statistical 
characteristics.
For example, an electroencephalogram(EEG) records
the electrical potential generated by the synchronized
firing of neurons. EEG signals are marked by both transient 
variations in waveforms and regime changes \textemdash distinct
breaks in the character and statistical distribution
of the EEG.
While the exact generating mechanisms of
variations in EEG may not be known, characteristics
 of the observed signal can be 
used to identify changes in the underlying dynamics.

The theory of $\varepsilon-$complexity as developed by 
Darkhovsky and Piryatinska provides a method
of identifying regime changes
in complex signals such as EEG.
The definition of $\varepsilon-$complexity is related 
to Kolmogorov's definition of the complexity of a sequence.
Roughly, Kolmogorov identified the complexity
of a discrete sequence as the size of the program, or the amount of information needed to produce that sequence. 
The $\varepsilon-$complexity of a continuous function, 
on the other hand, is the amount of information needed to reconstruct
that function with an absolute error not greater than 
$\varepsilon$. In the latter case, we consider the proportion 
of the function needed for reconstruction analogous
to information in Kolmogorov's theory.
Darkhovsky and Piryatinska have shown that the 
$\varepsilon-$complexity of a continuous function 
can be identified by two real numbers\cite{darkhovsky2013}. 
These are the 
$\varepsilon-$complexity coefficients, or simply the 
complexity coefficients of the function.
In practical applications, a signal like EEG is given by a
some finite set of samples. 
The theory of $\varepsilon-$complexity is adapted to a 
discrete setting by considering discrete sequences, such
as time series, as restrictions of continuous functions 
to a uniform grid.

The procedure for estimating the $\varepsilon-$complexity of a function entails iteratively approximating that
function on a more sparsely sampled set of points at each iteration. 
The initial implementation of the procedure used piecewise polynomials to approximate the original function. We implement the estimation procedure with an enlarged set of approximation methods \textemdash basis splines, cubic splines, and an interpolating subdivision method termed the lifting scheme. We test the estimation of 
$\varepsilon-$complexity on simulated data using each approximation
method. First, we compare the reconstruction accuracy of each method. Then we test the performance of the complexity coefficients estimated with each approximation method in classifying two groups of simulated time series.
We also compare the computational efficiency of each approximation method. Although the basis spline method has the lowest approximation error across all simulations, we find that low approximation error does not correspond to improvements in classification accuracy. Each 
approximation method performs similarly in the classification 
task. However, the cubic spline method is the most computationally efficient and is used in subsequent chapters to estimate the complexity coefficients.

Darkhovsky and Piryatinska have shown that for a H\"older class of functions the $\varepsilon$-complexity coefficients capture a linear relationship between the log of the approximation error and the log of the proportion of the function used in the approximation. They have conjectured that a 
constant generating mechanism corresponds to constant mean complexity coefficients. 
In Chapter 4 we use a number of simulations of deterministic and stochastic processes to test whether, on average, 
the complexity coefficients are constant for a 
constant H\"older class of functions. For these processes, a single parameter determines both the H\"older class and the fractal dimension of the time series generated
by the processes. For the tested simulations, the slope
complexity coefficient $B$ and an estimator
of fractal dimension exhibit similar behavior as the parameters of
the processes are varied. For three of the four processes tested,
we find that the complexity coefficient $B$ is
on average, constant for a constant H\"older class of functions.
 
% The characteristics of an EEG signal can change rapidly over a short time period. Common classification methods use features averaged over arbitrary windows of time. 

In the final chapter, the complexity coefficients are applied
to the prediction of seizures in epileptic mice. Features from a 4-minute window of EEG are used to predict whether a stimulus
applied to epilepsy-prone mice results in a seizure. 
% Most existing studies predict seizures with EEG data 
% from spontaneously occurring seizures. 
% For our study, we
A time series of length $n$ can be considered as a vector in an $n$-dimensional space. For our dataset, 4 minutes of 
EEG from a single channel is a represented by 2 million observations.
A common method of handling such high dimensional data is to map the data to a lower dimensional set of features. For time series like EEG, features are often calculated 
on a uniform partition or on a sliding
window taken over the length of the signal. However, EEG are  
inhomogeneous and this method of 
feature extraction may lead to features being calculated over
windows with widely varying characteristics.
In order to calculate our features on more homogeneous time periods, we use change points in the complexity coefficients to identify regime changes in the EEG signal. The EEG features are segmented based on change points in the complexity 
coefficients and average feature values on these segments are used as predictors of a seizure response. 

Models trained on features segmented in this manner are compared to several models trained on features calculated on uniform partitions of the EEG signal. 
The best performing models accurately classify seizure outcomes with over 80\% accuracy. A model with features calculated on uniform partitions performs 
as well or better than the models using changes in the complexity
coefficients to segment the signal.
Due to the small number of trials and some inherent limitations 
of the data, additional tests would be needed to 
know how well the model performance reported here generalizes
to other contexts.


Finally, we have created an \texttt{R} language package, 
\texttt{ecomplex}, that implements 
the $\varepsilon$-complexity algorithm\cite{aff2017}. The 
default settings of this package are based on the simulation 
experiments described in Chapter 3. Methods used 
for generating simulations, computing EEG features, 
and the classification algorithm used in Chapter 5 
have been also been included in \texttt{R} 
packages to enable further study or replication of
the experiments described in this work.


