% \documentclass[thesis]{subfiles}

% \begin{document}
% main.tex, to be used with thesis.tex
% This contains the main work of your thesis.

% \include{}

\chapter{Introduction}

% Time-series arising 
% in biomedical contexts are often complex and have
% underlying mechanisms that are not well understood. 
% Applications of classification and clustering arise
% naturally in this context; 
% For example, time series collected 
% from the electrical potential generated by the heart -- 
% electrocardiograms (ECG) -- or by the collective 
% firing of synapses as captured by an 
% electroencephalogram (EEG), 
% or generated by direct measurements of human motion,
% are commonly used to monitor and classify the health 
% of patients. 
% time series. 
% Although in practice the line between diagnosiing 
% and modeling a biological process may not be 
% precise, the distinction is useful when considering
% what feature or model of a time series are of 
% interest, as it mirrors the trade off that 
% is often made between interpretation and 
% accuracy in modeling and predicting phenomena.
%  The purely diagnostic setting corresponds
% closely to the algorithmic task of classifying 
% signals
% ==================================
%  old 
% ==================================
% A common practice is to model these time series using
% parametric models for which assumptions made about the
% nature of the process generating the data. However, 
% biophysical time series are often complex, non-stationary
% or have non-linear characteristics.
% parameters for the model are often either iteratively 
% tuned or are restricted to a specific range based
% on domain knowledge. 


Many natural phenomena generate time series that
exhibit complex functional and statistical 
characteristics.
% exhibit complex characteristics which are readily 
% represented by  statistical models.
For example, an electroencephalogram(EEG) records
the electrical potential generated by the synchronized
firing of neurons. EEG signals are marked by both transient 
variations in waveforms and regime changes, distinct
breaks in the character and statistical distribution
of the EEG.
While the exact generating mechanisms of
variations in EEG may not be known, characteristics
 of the observed signal can be 
used to identify changes in the underlying dynamics.

The theory of $\varepsilon-$complexity as developed by 
Darkhovsky and Piryatinska provides a method
of identifying these regime changes
in complex signals such as EEG.
The definition of $\varepsilon-$complexity is related 
to Kolmogorov's definition of the complexity of a sequence.
Roughly, Kolmogorov identified the complexity
of a discrete sequence as the size of the program, or 
the amount of information, needed to produce that sequence. 
The $\varepsilon-$complexity of a continuous function, 
on the other hand, is the amount of information --- in this 
case, the proportion of the function, needed to reconstruct
that function with an absolute error not greater than 
$\varepsilon$. 
Darkhovsky and Piryatinska have shown that the 
$\varepsilon-$complexity of a continuous function 
can be identified by two real numbers\cite{darkhovsky2013}. 
These are the 
$\varepsilon-$complexity coefficients or simply the 
complexity coefficients of the function.
In practical applications, a signal like EEG is given by a
some finite set of samples. 
The theory of $\varepsilon-$complexity is adapted to a 
discrete setting by considering discrete sequences, such
as time series, as restrictions of continuous functions 
to a uniform grid.

The procedure for estimating the $\varepsilon-$complexity of 
a function entails the iterative approximation of that 
function on a smaller set of samples at each iteration. 
The initial implementation of the procedure for estimating $\varepsilon-$complexity coefficients
used piecewise polynomials to reconstruct to approximate the 
original function. We implement the estimation procedure with an enlarged set of approximation methods --- basis splines, cubic splines, and an interpolating subdivision 
method termed the lifting scheme. We test the estimation of 
$\varepsilon-$complexity on simulated data using each approximation
method. First, we compare the reconstruction accuracy of of 
each method. Then we test the performance of the complexity coefficients estimated with each approximation 
method when used to classify groups of related time series.
We also compare the computational efficiency of each 
approximation method. Although the basis spline method has the 
lowest approximation error across all simulations, we 
find that low approximation error does not correspond 
to improvements in classification accuracy. Each 
approximation method performs similarly in the classification 
task. However, the cubic spline method is the most computationally efficient, and is used to estimate the complexity coefficients in 
the applications discussed in Chapters 4 and 5.

Darkhovsky and Piryatinska have shown that for a H\"older class of functions the $\varepsilon$-complexity coefficients capture a linear relationship between the log of the approximation error and the log of the proportion of the function used in the approximation. They have conjectured that a 
constant generating mechanism corresponds to constant mean complexity coefficients. 
In Chapter 4 we use a number of simulations of deterministic and stochastic processes to test whether, on average, 
the complexity coefficients are constant for a 
constant H\"older class of functions. For these processes, a single parameter determines both the H\"older class and the fractal dimension of the time series generated
by the processes. For these simulations, the slope
complexity coefficient $B$ and an estimator
of fractal dimension behave simlarly as the parameters of
the processes are varied. For three of the four processes tested,
we find that the complexity coefficient $B$ is
on average, constant for a constant H\"older class of functions.
 
% The characteristics of an EEG signal can change rapidly over a short time period. Common classification methods use features averaged over arbitrary windows of time. 

In the final chapter, the complexity coefficients are applied
to the prediction of seizures in epileptic mice. Features from a 4 minute window of EEG are used to predict whether a stimulus
applied to epilepsy-prone mice will result in a seizure. 
% Most existing studies predict seizures with EEG data 
% from spontaneously occuring seizures. 
% For our study, we
A time series of length $n$ can be considered 
as a vector in an $n$-dimensional space. For example, 4 minutes of 
EEG from a single channel is a represented by 2 million 
observations.
A common method of handling such high dimensional data is 
to map the data to a lower dimensional set of features. For time series like EEG, features are often calculated 
on a uniform partition or on a sliding
window taken over the length of the signal. For an 
inhomogeneous time series like EEG, 
this may lead to features being calculated windows
with widely varying characteristics.
The $\varepsilon-$complexity 
was designed to identify such variations in a signal.
In order to calculate our features on more homogeneous 
time periods, we use change points in the 
complexity coefficients to segment the EEG.
The average value of features on these segments are then used as predictors of a seizure response. 

Models using this procedure to dynamically segment the EEG signal
are compared to several models which use features calculated on uniform partitions of the signal. 
For the best performing models, we are able to accurately classify 
seizure outcomes with over 80\% accuracy. A model with features calculated on uniform partitions performs 
as well or better than the models using changes in the complexity
coefficients to segment the signal.
Due to the small number of trials and some inherent limitations 
of the data, additional tests would be needed to 
know if how well the model performance reported here generalizes
to other contexts.


Finally, we have created an \texttt{R} language package, 
\texttt{ecomplex}, that implements 
the $\varepsilon$-complexity algorithm. The 
default settings of this package are based on the simulation 
experiments described in Chapter 3. Methods used 
for generating simulations, computing EEG features, 
and the classification algorithm used in Chapter 5 
have been also been included in \texttt{R} 
packages to enable further study or replication of
the experiments described in this work.

% \section{The lifting scheme}
% % In theory, the minimum 
% % estimation error at each step given any family 
% % of functions used to interpolate (or approximate)
% % the original function.
% In theory, the approximation error found at each 
% stage of the algorithm should be all possible 
% families of approximating functions.
%  In practice, 
% some well-defined family of functions is used.
% We implement the $\epsilon-$complexity 
% algorithm using two related 
% methods: basis splines (B-splines)and
%  wavelet splines. The wavelet splines 
%  are implemented using a computationally 
% efficient method called a lifting scheme\cite{sweldens}. 
% One motivation for 
% extending the family of approximating functions is 
% to minimize the approximation error at each 
% stage of the algorithm. Wavelet splines, though, are 
% essentially an iterated version of basis spline
% interpolation. That is, a single iteration of the 
% wavelet spline interpolation is equivalent to 
% interpolation using B-splines.
% The resulting reconstruction error does not differ
% significantly from the B-spline implementation. 
% However, the listing scheme 
% provides an time and space efficient method 
% for computing the interpolation. 

% % Since the spacing 
% % of the points used for interpolation are known 
% % at each stage of the algorithm a discrete time 
% %  filter can be computed before hand,
% % allowing for an efficient computation of the 
% % interpolated points.
% % (Specific claims f a faster implementation to 
% % be detailed later).


% % (transition)

% \section{Classification of EEG}

% % The $\epsilon$-complexity algorithm relies on
% % repeated reconstruction of the original data at  
% % coarser levels, or repeated downsampling of 
% % the data. This makes it best suited for 
% % densely sampled data. 

 
% % mice with a genetic disorder that makes them 
% % prone to epilepsy. 

% % (Explanation)
% The EEG signal measures the changes 
% in the brain's electrical field potential. 
% EEG signals are used in a range 
% of biomedical contexts, from diagnosing
% diseases of the brain to medical
% and commercial brain-computer applications.
% Traditional
% EEG analysis has focused on the spectral
% characteristics of the EEG signal. 
% A typical analysis characterizes a brain 
% state by the relative power within a given 
% frequency band. For example, low frequency 
% delta waves (0.5-4 Hz) and mid-frequency 
% alpha waves (8-12 Hz) have been associated 
% with distinct sleep stages.
%  However, EEG signals exhibit a range of 
%  complex characteristics. For example, 
%  EEG are non-homogeneous, non-stationary and have transient 
%  waveforms. \cite{pirya2009}
%  These
% characteristics make it difficult to analyze
% using standard parametric time series models.\cite{subha} 
% In addition to spectral characteristics 
% a range of features drawn from the study dynamic
% and non-linear systems -- fractional dimension, 
% correlation dimension, and higher order spectra
% have been used to study or classify EEG 
% signals.\cite{subha} 

% In this thesis we use a combination of these  
% features to study EEG signals of mice with 
% a genetic mutation that causes epilepsy. 
% We use the $\epsilon-$complexity algorithm 
% combined with spectral features and variance
% features to segment and classify the EEG 
% signals. In particular, we (attempt) to 
% predict the reaction of the mice to a stimulus
% intended to induce an epileptic reaction. 
% We use a change point algorithm to segment 
% data and then extract features from related 
% segments. The features drawn from this segmented
% data are then used to predict the post stimulus
% brain state of the mice.

% \section{The \texttt{ecomplex R} package}

% We have written an \texttt{R} package that includes 
% the $\epsilon-$complexity algorithm.  
% The package also the change point detection 
% algorithm used in our EEG analysis and  
% described in \cite{pirya2009}.[update] 
% A separate package includes code that 
% allows for the replication of our 
% analysis of synthetic time series and 
% mouse EEG data.


% We begin by outlining the motivation for the 
% $\epsilon-$complexity algorithm and we develop
% the mathematical theory of the procedure. We 
% review several features
% that aim to quantify 'complexity', 
% and describe their relation to the $\epsilon$-complexity
% feature. The mathematical background of 
% lifting scheme interpolation and it's relation to 
% wavelets is described along with the details 
% of the algorithm's implementation in the 
% \texttt{ecomplex} package. The new implementation 
% is tested on a set of synthetic data. Finally, 
% an analysis and classification of the 
%  the mutant epileptic mouse EEG 
% carried out using a change point algorithm and 
% a set of additional features including 
% measures of the variance and spectral content
% of the signal.


